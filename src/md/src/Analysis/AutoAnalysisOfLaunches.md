## Auto-Analysis of launches

The analysis feature of the ReportPortal makes it possible for the application to check and pass part of the routine duties by itself.

Auto-analysis take a part of your routine work and defines the reason for the test item failure and sets:

* a defect type;
* a link to BTS _(in case if it exists)_;
* comment _(in case if it exists)_;

The process of Auto-Analysis is based on previous user-investigated users' results using Machine Learning.  

An auto-analyzer is presented by a combination of several services: ElasticSearch, Analyzer service(two instances Analyzer and Analyzer train), Metrics gatherer:
* Elasticsearch contains an analytical base, stores training data for retraining of models and saves metrics for metrics gatherer.
* Analyzer instance performs all operations, connected with the basic functionality (indexing/removing logs, searching logs, auto-analysis,  ML suggestions).
* Analyzer train instance is responsible for training models for Auto-analysis and ML suggestions functionality.
* Metrics gatherer calculates metrics about the analyzer usage and requests deletion of custom models if metrics goes down.

There are several ways to use an analyzer in our application:

* Use  the ReportPortal Analyzer: **manual** (analysis is switched on only for chosen launch manually) or **auto** (analysis is switched on after the launch finishing automatically);

* Implement and configure your custom Analyzer and do not deploy ReportPortal service Analyzer;

* Do not use any Analyzers at all and do an analytical routine by yourself;

### ReportPortal Analyzer. How to install

* Add info about Service Analyzer and service ElasticSearch in the docker-compose file;

* Set {vm.max_map_count} kernel setting before ReportPortal deploying with [command>](https://www.elastic.co/guide/en/elasticsearch/reference/6.1/docker.html#docker-cli-run-prod-mode);

* Give right permissions to ElasticSearch data folder using the following command:
```javascript
mkdir data/elasticsearch
```
```javascript
chmod g+rwx data/elasticsearch
```
```javascript
chgrp 1000 data/elasticsearch
```

For more details about ElasticSearch visit [ElasticSearch guide](https://www.elastic.co/guide/en/elasticsearch/reference/6.1/docker.html#_notes_for_production_use_and_defaults);

### ReportPortal Analyzer. How the Auto-Analysis is working

ReportPortal's auto-analyzer allows users to reduce the time spent on test execution investigation by analyzing test failures in automatic mode. For that reason, you can deploy the  ReportPortal with a service Analyzer by adding info about this service in a docker-compose file. The default analysis component is running along with ElasticSearch which is used for test logs indexing.
For effective using Auto–Analysis you should come through several stages. 

#### Create an analytical base in the ElasticSearch 

First of all, you need to create an analytical base. For that, you should start to analyze test results manually. 

All test items with a defect type which have been analyzed manually or automatically by ReportPortal are sent to the Elastic Search. 

The following info is sent:

* An item ID;
* Logs (each log should be with level Error and higher (log level >= 40 000));
* Issue type;
* Flag: “Analyzed by” (where shows by whom the test item has been analyzed by a user or by ReportPortal);
* A launch name;
* Launch ID;
* Unique ID;
* Test case ID;

For the better analysis, we merge small logs (which consist of 1-2 log lines and words number <= 100) together. We store this merged log message as a separate document if there are no other big logs (consisting of more than 2 log lines or having a stacktrace) in the test item. We store this merged log message in a separate field "merged_small_logs" for all the big logs if there are ones.

The Analyzer preprocesses log messages from the request for analysis: extracts error message, stacktrace, numbers, exceptions, urls, paths, parameters and other parts from text to search for the most similar items by these parts in the analytical base. These parts are saved in a separate fields for each log entry.

Each log entry along with its defect type is saved to ElasticSearch in the form of a separate document. All documents created compose an Index. The more test results index has, the more accurate results will be generated by the end of the analysis process.
>
>If you do not sure how many documents(logs) are contained in the Index at that moment, you can check it. 
>For that, perform the following actions: 
>* Uncommented Service ElasticSearch ports in a docker-compose file or add them: 9200:9200; 
>* Restart-Service ElasticSearch with new docker-compose; 
>* Send a request to ElasticSearch: 
>    * how many documents in the Index: GET http://localhost:9200/_cat/indices?v  
>    * Detailed information:  POST http://localhost:9200/{project_name}/_search

Test items of a launch in Debug mode are not sent to the service Analyzer. If the test item is deleted or moved to the Debug mode, it is removed from the Index.

#### Auto-Analysis process

After your Index has been completed. You can start to use the auto-analysis feature.

Analysis can be launched automatically (via Project Settings) or manually (via the menu on All launches view). After the process is started, all items with defect type “To investigate” with logs (log level >= 40 000) from the analyzed launch are picked and sent to the Analyzer Service and the service ElasticSearch for investigations.

#### How Elasticsearch returns candidates for Analysis
Here is a simplified procedure of the Auto-analysis candidates searching via ElasticSearch.

When a "To investigate" test item appears we search for the most similar test items in the analytical base. We create a query which searches by several fields, message similarity is a compulsory condition, other conditions boost the better results and they will have a higher score (boost conditions are similarity by unique id, launch name, error message, found exceptions, numbers in the logs and etc.). 

Then ElasticSearch receives a log message and divides it into the terms (words) with a tokenizer and calculates the importance of each term (word). For that ElasticSearch computes TF-IDF for each term (word) in the analyzed log. If the level of term importance is low, the ElasticSearch ignores it. 

>**Note:**
>
>*Term frequency (TF)* – how many time term (word) is used in an analyzed log;
>
>*Document frequency (DF)* – in how many documents this term (word) is used in Index;
>
>*TF-IDF (TF — term frequency, IDF — inverse document frequency)* — a statistical measure used to assess the importance of a term (word) in the context of a log that is part of an Index. The weight of a term (word)   is proportional to the amount of use of this term (word)   in the analyzed log and inversely proportional to the frequency of term (word)   usage in Index.

The term (word) with the highest level of importance is the term (word) that is used very frequently in analyzed log and moderately in the Index.
 
After all important terms are defined, Elastic search calculates the level of equality between an analyzed log and each log in the Index.  For each log from the Index is calculated a score. 

>**Note:**
>
>How calculated a score:
>
>**score(q,d)** = 
>
>         coord(q,d) -
>         SUM ( 
>                tf(t in d),  
>               idf(t)²,  
>                t.getBoost(), 
>                       ) (t in q)
>Where:
>*    score(q,d) is the relevance score of log “d” for query “q”.
>*    coord(q,d) is the coordination factor: the percent of words equality between analyzed log and particular log from the ElasticSearch.
>*    The sum of the weights for each word “t” in the query “q” for log “d”.
>    * tf(t in d) is a frequency of the word in the analyzed log.
>    * idf(t) is the inverse frequency of the word in all saved logs in the Index.
>    * t.getBoost() is the boost that has been applied to the query. The higher priority for logs with:
>       * The same Launch name;
>       * The same UID;
>       * Manual analysis;
>       * Error message;
>       * The same numbers in the log;
>       * and etc.

The results are sorted by the score, in case the scores are the same, they are sorted by "start_time" field, which helps to boost the test items with closer to today dates. So the latest defect types will be higher in the returned by Elasticsearch results.

The ElasticSearch returns to the service Analyzer 10 logs with the highest score for each log. Analyzer regroups all the results by a defect type and chooses the best representative for each defect type group, based on their scores.

>**Note:**
In the case the test item has several logs, the best representative for a defect type group will become the log with the highest score among all logs.

#### How Auto-analysis makes decisions for candidates, returned by Elasticsearch

The ElasticSearch returns to the service Analyzer 10 logs with the highest score for each query and all these candidates will be processed further by the ML model. Analyzer regroups all the results by a defect type and chooses the best representative for each defect type group, based on their scores.

The ML model is an XGBoost model which features (about 30 features) represent different statistics about the test item, log message texts, launch info and etc, for example:
* the percent of selected test items with the following defect type
* max/min/mean scores for the following defect type
* cosine similarity between vectors, representing error message/stacktrace/the whole message/urls/paths and other text fields
* whether it has the same unique id, from the same launch
* the probability for being of a specific defect type given by the Random Forest Classifier trained on Tf-Idf vectors

The model gives a probability for each defect type group, and we choose the defect type group with the highest probability and the probability should be >= 50%.

A defect comment and a link to BTS of the best representative from this group come to the analyzed item.

The Auto-analysis model is retrained for the project and this information can be found in the section "How models are retrained" below.

So this is how Auto-Analysis works and defines the most relevant defect type on the base of the previous investigations. We give an ability to our users to configure auto-analysis manually.

### Auto-analysis Settings

All settings and configurations of Analyzer and ElasticSearch are situated on a separate tab on Project settings.

1. Login into ReportPortal instance as Administrator or project member with PROJECT MANAGER or LEAD role on the project;

2. Come on Project Settings, choose Auto-Analysis section;

[ ![Image]( Images/userGuide/analyzeLaunches/AnlyzerClassic.png) ]( https://https://youtu.be/7dN8uEKTkzo)

In this section user can perform the following actions:

1.    Switch ON/OFF auto-analysis;

2.    Choose a base for analysis (All launches/ Launches with the same name);

3.    Configure ElasticSearch settings;

4.    Remove/Generate ElasticSearch index.


#### Switch ON/OFF automatic analysis;

To activate the "Auto-Analysis" functionality in a project, perform the following steps:

1. Login ReportPortal instance as Administrator or project member with PROJECT MANAGER or LEAD role on the project.

2. Select ON  in the "Auto-Analysis" selector on the Project settings / Auto-analysis section.

3. Click the "Submit" button. Now "Auto-Analysis" will start as soon as any launch finishes.

#### Base for analysis (All launches/ Launches with the same name);

You can choose which results from previous runs should be considered in Auto –Analysis for defining the failure reason.

There two options:

* All launches;

* Launches with the same name;

If you choose **“All launches”**, test results in the launch will have analyzed on the base of all data in Elastic search of the project. 

If you choose **“Launches with the same name”**, test results in the launch will have analyzed on the base of all data in Elastic search that have the same Launch name.

You can choose those configurations via Project configuration or from the list of actions on All launches view.

#### Configure ElasticSearch settings

Also, we give the possibility for our users to configure 2 main parameters of ElasticSearch manually:

*    **MinShouldMatch** - percent of words equality between analyzed log and particular log from the ElasticSearch. If a log from ElasticSearch has the value less then set, this log will be ignored for AA. Min value 50, max value 100.
*    **Number of log lines** - the number of first lines of log message that should be considered in ElasticSearch. Only the chosen number of logline will be saved in ElasticSearch. Possible values: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, All.  In case you choose “ALL”, the full text of log will be saved in ElasticSearch Index.

Parameter **MinShouldMatch** is involved in the calculation of a score. It is a minimum value for coord(q,d) (the percent of words equality between an analyzed log and a particular log from the ElasticSearch). So you can increase search hardness and choose a minimum level of similarity that is required.

With the parameter **Number of log lines** - you can write the root cause of test failure in the first lines and configure the analyzer to take into account only the required lines. 

With these 2 parameters, you can configure the accuracy of the analysis that you need. For your facilities we have prepared 3 pre-sets with values:

*    *Light* - search conditions are freer. You will get more results, but with the less level of similarity;
*    *Moderate* - "happy medium";
*    *Classic* - search conditions are strict. You will get fewer results, but with a higher level of similarity;

#### Remove/Generate ElasticSearch index

There two possible actions that can be performed under Index in ElasticSearch. 

You can **remove the Index from ElasticSearch** and all logs with there defect type will be deleted. ML will be set to zero. All data with your investigations will be deleted from the ElasticSearch. For creating a new one you could start to investigate test results manually or generate data based on previous results on the project once again. 

>**Note:**
Your investigations in ReportPortal will not be changed. The operation concerns only ElasticSearch base.

[ ![Image]( Images/userGuide/analyzeLaunches/AAremove.png) ]( https://youtu.be/lZoZm_n4vNw)

Another option, you can **generate the Index in ElasticSearch**. In the case of generation, all data will be removed from ElasticSearch and the new one will be generated based on all previous investigations on the project following current analysis settings. 

At the end of the process, you will receive a letter with info about the end of the process and with several items that will be appeared in ElasticSearch.

You can use index generation for several goals. For example, assume two hypothetical situations when index generation can be used:

* by accident you remove the index, but now you want to restore it. 

>**Note:** 
The new base will be generated following logs and settings that are existing on the moment of operating. So index before removing and index after generation can be different. 

* you have changed a parameter **Number of log lines** for 3.  But your existing index contains logs with value ALL. You can generate a new index, the old index will be removed, and a new one will be generated. Logs in the new index will contain 3 lines;  

[ ![Image]( Images/userGuide/analyzeLaunches/AAgenerate.png) ]( https://https://youtu.be/54EO4qM-9HU)

We strongly do not recommend use auto-analysis until the new index will be generated.

#### Manual analysis

Analysis can be launched manually. To start the analysis manually, perform the following steps:

1. Navigate to the "Launches" page.

2. Select the "Analysis" option from the context menu next to the selected launch name.

3. Choose the scope of previous results on the base of which test items should be auto-analyzed.  The default is the one that is chosen on the setting page, but you can change it manually.

Via this menu you can choose 3 options unlike on Project Settings: 

* All launches;

* Launches with the same name;

* Only current launch;

Options **All launches** and **Launches with the same name** are working the same as on project settings. 
If you choose **Only current launch**, the system is analyzing the test items of chosen launch only on a base of already investigated date of this launch.

4. Choose which items from launch should be analyzed:

* Only To investigated; 
* Items analyzed automatically (by AA);
* Items analyzed manually;

In case the user chooses **Only To investigate items** -  the system is analyzing only items with defect type "To investigate" in the chosen launch;

In case the user chooses **Items analyzed automatically (by AA)** - the system is analyzing only items that have been already analyzed by auto-analysis. The results of the previous run of analysis will be set to zero and items will be analyzed once again.

In case the user chooses **Items analyzed manually** - the system is analyzing only items that have been already analyzed by the user manually. The results of the previous run of analysis will be set to zero and items will be analyzed once again.

In the case of multi-combination - the system is analyzing results dependence on chosen options.

>**Note:**
The Ignore flag is saved. If the item has flag **Ignore in AA**, it will not be re-analyzed.

> **Note:**
For option **Only current lunch** you can not choose *Items analyzed automatically (by AA)* and *Items analyzed manually* simultaneously.

4. Click the "Analysis" button. Now "Auto-Analysis" will start.

Any launches with an active analyzing process will be marked with the "Analysis" label. 

 [ ![Image]( Images/userGuide/analyzeLaunches/AAmanual.png) ](https://youtu.be/ulJ16fRT2Jw)


### Label AA

When the test item is analyzed by the ReportPortal, a label "AA" is set on the test item on a Step Level. You can filter results with a parameter “Analysed by RP (AA)”

[ ![Image](Images/userGuide/analyzeLaunches/Auto-Analysis-AA.png) ](Images/userGuide/analyzeLaunches/Auto-Analysis-AA.png)

### Ignore in Auto-Analysis

If you don't want to save some test items in ElasticSearch, you can "Ignore it in Auto-Analysis". For that you can choose this action in “Defect type editor” pop-up:

[![Image](Images/userGuide/analyzeLaunches//Auto-Analysis-IgnorePopUp.png) ](Images/userGuide/analyzeLaunches/Auto-Analysis-IgnorePopUp.png)

Or from the action list for several test items:

[ ![Image](Images/userGuide/analyzeLaunches/Auto-Analysis-IgnoreActionList.png) ](Images/userGuide/analyzeLaunches/Auto-Analysis-IgnoreActionList.png)

When you choose “Ignore in AA”, logs of the chosen item are removed from the ElasticSearch. 



## Search for the similar "To investigate" items

**Use case:**

**Situation:** Analyzer has completed its work and marked known issues with defect types.

But there are a lot of failures with a similar unknown reason in the run. All such items have "To investigate" defect type.

**Problem:** A user should check and analyze all failed items.

**Solution:**

A user is on All launches, he clicks on "To investigate" and opens a list with items. When a user clicks on a pencil next to a defect type, the system opens the ["Make decision" modal](https://reportportal.io/docs/Manual-Analysis%3E-make-decision-modal-redesign). In this modal a user can see all items with "To investigate" defect type and the same failure reason.

There are 3 options for search the similar "To investigate" items on the Step level:

- Current item only
- Similar "To investigate" in the launch & current item
- Similar "To investigate" in 10 launches & current item

There are 4 options for search the similar "To investigate" items on the Log level:

- Current item only
- Similar "To investigate" in the launch & current item
- Similar "To investigate" in 10 launches & current item
- "To investigate" from the history line & current item

If launches are filtered by filter on All Launches page, then addition option ```Similar "To investigate" in the Filter & current item``` appears on the Step and Log levels.

[ ![Image](Images/userGuide/analyzeLaunches/SearchSimilarToInvestigate1.png) ](Images/userGuide/analyzeLaunches/SearchSimilarToInvestigate1.png)

A user can select all identical failures and perform the bulk operation for them.

[ ![Image](Images/userGuide/analyzeLaunches/SearchSimilarToInvestigate2.png) ](Images/userGuide/analyzeLaunches/SearchSimilarToInvestigate2.png)



## Copy results from a previous run

In case you do not want to use Auto-Analyzer we provide you a possibility to get results from previous runs.  So that you can set for a test item: Defect type, linked bug, and comment from the previous run at once. For that, you can hit a button "Copy defect from #" in the last test item and copy a defect from the last but one test item with defect type.

[ ![Image](Images/userGuide/analyzeLaunches/CopyResults.png)](Images/userGuide/analyzeLaunches/CopyResults.png)

or hit a button "Send defect to #" from the not the last items and send defect to the last test item ( if it can have a defect type).

[ ![Image](Images/userGuide/analyzeLaunches/SendResults.png)](Images/userGuide/analyzeLaunches/SendResults.png)

## ML Suggestions

ML suggestions functionality is based on previously analyzed results (either manually or via Auto-analysis feature) using Machine Learning. The functionality is provided by the Analyzer service in combination with ElasticSearch. All the installations steps can be found in the "ReportPortal Analyzer. How to install" section above.

This analysis hints what are the most similar analyzed items to the current test item. You can interact with this functionality in several ways:
* Choose one of the suggested items if you see that the reason for the current test item is similar to the suggested one. When you choose the item and apply changes to the current item, the following test item characteristics will be copied from the chosen test item:
    * a defect type;
    * a link to BTS _(in case if it exists)_;
    * comment _(in case if it exists)_;

* If you see no suitable suggested test item for the current test item, just do not select any of them.

### How the ML suggestions functionality is working

ML Suggestions searches for similar previously analyzed items to the current test item, so it requires an analytical base saved in Elasticsearch. How an analytical base is created you can find in the subsection "Create an analytical base in the ElasticSearch" above. ML suggestions takes into account all user-investigated, auto-analyzed items or items chosen from ML suggestions. While the analytical base is growing ML suggestions functionality will have more examples to search by and suggest you the best options.

ML suggestions analysis is run everytime you enter "Make decision" editor. ML suggestions are run for all test items no matter what defect type they have now. This functionality is processing only test items with logs (log level >= 40000). 

The request for the suggestions part looks like this: 
* testItemId;
* uniqueId;   
* testCaseHash;        
* launchId; 
* launchName;    
* project; 
* analyzerConfig;
* logs = List of log objects (logId, logLevel, message)

The Analyzer preprocesses log messages from the request for analysis: extracts error message, stacktrace, numbers, exceptions, urls, paths, parameters and other parts from text to search for the most similar items by these parts in the analytical base. We make several requests to the Elasticsearch to find similar test items by all the error logs. **Note:** When a test item has several error logs, we will use the log with the highest score as a representative of this test item.

The ElasticSearch returns to the service Analyzer 10 logs with the highest score for each query and all these candidates will be processed further by the ML model. The ML model is an XGBoost model which features (about 40 features) represent different statistics about the test item, log message texts, launch info and etc, for example:
* the percent of selected test items with the following defect type
* max/min/mean scores for the following defect type
* cosine similarity between vectors, representing error message/stacktrace/the whole message/urls/paths and other text fields
* whether it has the same unique id, from the same launch
* the probability for being of a specific defect type given by the Random Forest Classifier trained on Tf-Idf vectors

The model gives a probability for each candidate, we filter out test items with the probability <= 40%. We sort the test items by this probability, after that we deduplicate test items inside this ranked list. If two test items are similar with >= 98% by their messages, then we will leave the test item with the highest probability. After deduplication we take maximimum 5 items with the highest score to show in the ML Suggestions section.

ML suggestions section contains at maximum 5 suggested items, they are shown together with the scores given by the model and we divide them into 3 groups:
* the group "SAME", test items with the score = 100% 
* the group "HIGH", test items with the score in the range [70% - 99.9%]
* the group "LOW", test items with the score in the range [40% - 69.9%]

## How models are retrained

In the Auto-analysis and ML suggestions processes several models take part: 
* Auto-analysis XGBoost model, which gives the probability for a test item to be of a specific type based on the most similar test item in the history with this defect type
* ML suggestions XGBoost model, which gives the probability for a test item to be similar to the test item from the history
* Error message language model on Tf-Idf vectors(Random Forest Classifier), which gives a probability for the error message to be of a specific defect type or its subtype based on the words in the message. The probability from this model is taken as a feature in the main boosting algorithm.

At the start of the project, you have global models. They were trained on 6 projects and were validated to give a good accuracy on average. To have a more powerful and personalized analysis, the models should be retrained on the data from the project. **Note:** If a global model performs better on your data, the retrained model won't be saved. As far as we save a custom model only if it performs better for your data than the global one.

Triggering information and retrained models are saved in Minio(or a filesystem) as you set up in the Analyzer service settings.

Retraining triggering conditions for **Error message Random Forest Classifier**:
* Each time the test item defect type is changed to another issue type(except "To Investigate"), we update the triggering info, which saves the quantity of test items with defect types and the quantity of test items with defect types since the last training. This information is saved in the file "defect_type_trigger_info" in Minio. 
* When we have more than 100 labelled items, and since last training we have 100 labelled test items, retraining is triggered and if validation data metrics are better than metrics for a global model for the same data points, then we save a custom "defect_type" model in Minio and use it further in the auto-analysis and suggestions functionality.

Retraining triggering conditions for **Auto-analysis** and **Suggestion XGBoost models**:
* We gather training data for training from several sources:
    * when you choose one of the suggestions(the chosen test item will be a positive example, others will be negative ones)
    * when you don't choose any suggestion and edit the test item somehow(set up a defect type manually, add a comment, etc.), all suggestions become negative examples;
    * when auto-analysis runs and for a test item it finds a similar test item, we consider it a positive example, until the user changes the defect type for it manually. In this case, the result will be marked as a negative one.
* Each time a suggestion analysis runs or changing a defect type happens, we update the triggering info for both models. This information is saved in the files  "auto_analysis_trigger_info" and "suggestion_tgrigger_info" in Minio.
* When we have more than 300 labelled items, and since last training we have 100 labelled test items, retraining is triggered and if validation data metrics are better than metrics for a global model for the same data points, then we save a custom "auto_anlysis" model in Minio and use it further in the auto-analysis functionality.
* When we have more than 100 labelled items, and since last training we have 50 labelled test items, retraining is triggered and if validation data metrics are better than metrics for a global model for the same data points, then we save a custom "suggestion" model in Minio and use it further in the suggestions functionality.
